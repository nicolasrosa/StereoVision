% Referências Definitivas
@article{Dissanayake2001,
abstract = {The simultaneous localization and map building (SLAM) problem asks$\backslash$nif it is possible for an autonomous vehicle to start in an unknown$\backslash$nlocation in an unknown environment and then to incrementally build a map$\backslash$nof this environment while simultaneously using this map to compute$\backslash$nabsolute vehicle location. Starting from estimation-theoretic$\backslash$nfoundations of this problem, the paper proves that a solution to the$\backslash$nSLAM problem is indeed possible. The underlying structure of the SLAM$\backslash$nproblem is first elucidated. A proof that the estimated map converges$\backslash$nmonotonically to a relative map with zero uncertainty is then developed.$\backslash$nIt is then shown that the absolute accuracy of the map and the vehicle$\backslash$nlocation reach a lower bound defined only by the initial vehicle$\backslash$nuncertainty. Together, these results show that it is possible for an$\backslash$nautonomous vehicle to start in an unknown location in an unknown$\backslash$nenvironment and, using relative observations only, incrementally build a$\backslash$nperfect map of the world and to compute simultaneously a bounded$\backslash$nestimate of vehicle location. The paper also describes a substantial$\backslash$nimplementation of the SLAM algorithm on a vehicle operating in an$\backslash$noutdoor environment using millimeter-wave radar to provide relative map$\backslash$nobservations. This implementation is used to demonstrate how some key$\backslash$nissues such as map management and data association can be handled in a$\backslash$npractical environment. The results obtained are cross-compared with$\backslash$nabsolute locations of the map landmarks obtained by surveying. In$\backslash$nconclusion, the paper discusses a number of key issues raised by the$\backslash$nsolution to the SLAM problem including suboptimal map-building$\backslash$nalgorithms and map management},
author = {Dissanayake, M.W.M.G. and Newman, P. and Clark, S. and Durrant-Whyte, H.F. and Csorba, M.},
doi = {10.1109/70.938381},
isbn = {1042-296X},
issn = {1042-296X},
journal = {IEEE Transactions on Robotics and Automation},
number = {3},
pages = {229--241},
title = {{A solution to the simultaneous localization and map building (SLAM)}},
volume = {17},
year = {2001}
}


@misc{AAVC,
mendeley-groups = {TCC},
title = {{FLYAAVC. Autonomous Aerial Vehicle Competition}},
howpublished={\url{http://www.flyaavc.org/}},
year={Acesso em: 09 de Novembro de 2015}
}


@article{Lemaire2007,
author = {Lemaire, Thomas and Berger, Cyrille and Jung, Il-Kyun and Lacroix, Simon},
doi = {10.1007/s11263-007-0042-3},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {3D SLAM,bearing only SLAM,interest point matching},
language = {English},
number = {3},
pages = {343--364},
publisher = {Kluwer Academic Publishers-Plenum Publishers},
title = {{Vision-Based SLAM: Stereo and Monocular Approaches}},
url = {http://dx.doi.org/10.1007/s11263-007-0042-3},
volume = {74},
year = {2007}
}


@article{Shah2014,
abstract = {In 2012 a federal mandate was imposed that required the FAA to integrate unmanned aerial systems (UAS) into the national airspace (NAS) by 2015 for civilian and commercial use. A significant driver for the increasing popularity of these systems is the rise in open hardware and open software solutions which allow hobbyists to build small UAS at low cost and without specialist equipment. This paper describes our work building, evaluating and improving performance of a vision-based system running on an embedded computer onboard such a small UAS. This system utilises open source software and open hardware to automatically land a multi-rotor UAS with high accuracy. Using parallel computing techniques, our final implementation runs at the maximum possible rate of 30 frames per second. This demonstrates a valid approach for implementing other real-time vision based systems onboard UAS using low power, small and economical embedded computers.},
author = {Shah, Sunil},
file = {:media/nicolas/Documentos/Google Drive/TCC/Books/Disserta{\c{c}}{\~{o}}es/EECS-2014-117.pdf:pdf},
mendeley-groups = {TCC},
pages = {1--29},
title = {{Real-time Image Processing on Low Cost Embedded Computers}},
year = {2014}
}


@book{Bradski2008,
abstract = {Learning OpenCV puts you right in the middle of the rapidly expanding field of computer vision. Written by the creators of OpenCV, the widely used free open- source library, this book introduces you to computer vision and demonstrates how you can quickly build applications that enable computers to "see" and make decisions based on the data. Computer vision is everywhere - in security systems, manufacturing inspection systems, medical image analysis, Unmanned Aerial Vehicles, and more. It helps robot cars drive by themselves, stitches Google maps and Google Earth together, checks the pixels on your laptop's LCD screen, and makes sure the stitches in your shirt are OK. OpenCV provides an easy-to-use computer vision infrastructure along with a comprehensive library containing more than 500 functions that can run vision code in real time. With Learning OpenCV, any developer or hobbyist can get up and running with the framework quickly, whether it's to build simple or sophisticated vision applications. The book includes: A thorough introduction to OpenCV Getting input from cameras Transforming images Shape matching Pattern recognition, including face detection Segmenting images Tracking and motion in 2 and 3 dimensions Machine learning algorithms Hands-on exercises at the end of each chapter help you absorb the concepts, and an appendix explains how to set up an OpenCV project in Visual Studio. OpenCV is written in performance optimized C/C++ code, runs on Windows, Linux, and Mac OS X, and is free for commercial and research use under a BSD license. Getting machines to see is a challenging but entertaining goal. If you're intrigued by the possibilities, Learning OpenCV gets you started onbuilding computer vision applications of your own.},
author = {Bradski, Gary and Kaehler, Adrian},
booktitle = {OReilly Media Inc},
doi = {10.1109/MRA.2009.933612},
isbn = {0596516134},
issn = {10709932},
mendeley-groups = {TCC},
pages = {555},
title = {{Learning OpenCV: Computer Vision with the OpenCV Library}},
url = {http://www.amazon.com/dp/0596516134},
volume = {1},
year = {2008}
}


@article{ShinzatoP,
author = {{Shinzato, P. Y. ; Os{\'{o}}rio, F. S. ; Wolf}, D. F.},
journal = {IEEE/RSJ International Conference on Intelligent Robots and Systems - IROS - Workshop},
mendeley-groups = {TCC},
title = {{Visual Road Recognition Using Artificial Neural Networks and Stereo Vision.}}
}


@article{BarryMIT,
author = {Barry, Andrew J and Oleynikova, Helen and Honegger, Dominik and Pollefeys, Marc and Tedrake, Russ},
file = {:media/nicolas/Documentos/Google Drive/TCC/Books/Artigos/Richard Szeliski - Computer Vision Algorithms and Applications.pdf:pdf},
journal = {Vision-based Control and Navigation of Small Lightweight UAVs, IROS Workshop},
mendeley-groups = {TCC},
pages = {1--6},
title = {{FPGA vs . Pushbroom Stereo Vision for MAVs}},
year = {2015}
}


@article{Barry2015,
abstract = {As MAVs increase in functionality and utility, lightweight perception becomes critical to increasing autonomy. Cameras present an attractive solution for on-board sensing, due to their low weight, small power consumption, and dense information stream. High-speed stereo vision provides 3D data for obstacle avoidance, but generally has substantial processing requirements which are especially difficult to achieve on small aircraft. Here, we compare two solutions to this problem: dense stereo on an FPGA and sparse stereo on an ARM processor. We detail the design considerations and performance of both systems on duplicate fixed-wing aircraft platforms flying near obstacles in an outdoor environment.},
author = {Barry, Andrew J. and Oleynikova, Helen and Honegger, Dominik and Pollefeys, Marc and Tedrake, Russ},
file = {:home/nicolas/Desktop/Barry15a.pdf:pdf},
mendeley-groups = {TCC},
pages = {7},
title = {{Fast Onboard Stereo Vision for UAVs}},
url = {http://groups.csail.mit.edu/robotics-center/public{\_}papers/Barry15a.pdf},
year = {2015}
}


@article{Nagappa2013,
abstract = {This paper considers the application of feature- based simultaneous localisation and mapping (SLAM) using a random finite sets (RFS) framework for an autonomous underwater vehicle. SLAM allows for reduction in localisation error by tracking features which provide a fixed external reference. The SLAM problem is addressed here using a single- cluster probability hypothesis density (PHD) filter. The filter uses a particle approximation for the vehicle position with a conditional Gaussian mixture PHD for the feature map. Map features are selected as unique point features generated from a stereo camera on-board the vehicle. We demonstrate the improvement in localisation applying the algorithm to a dataset obtained in an indoor test tank. I.},
author = {Nagappa, Sharad and Palomeras, Narc{\'{\i}}s and Lee, Chee Sing and Gracias, Nuno and Clark, Daniel E. and Salvi, Joaquim},
doi = {10.1109/OCEANS-Bergen.2013.6608107},
file = {:media/nicolas/Documentos/Google Drive/TCC/Books/Artigos/Nagappa13{\_}oceans{\_}130118-113.pdf:pdf},
isbn = {978-1-4799-0001-5},
journal = {2013 MTS/IEEE OCEANS-Bergen},
keywords = {SLAM,underwater,vision},
mendeley-groups = {TCC},
number = {Ref 288273},
pages = {1--9},
title = {{Single cluster PHD SLAM: Application to autonomous underwater vehicles using stereo vision}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6608107},
year = {2013}
}


@misc{UEDA2011,
author = {UEDA, Etsuko; and Koeda, Masanao; and Suenaga, Tsuyoshi; and Takemura, Kentaro;},
booktitle = {Jan 05, 2011},
mendeley-groups = {TCC},
title = {{3D Reconstruction and Point Cloud Rendering}},
howpublished={\url{http://opencv.jp/opencv2-x-samples/point-cloud-rendering}},
year = {Acesso em: 05 de Dezembro de 2015}
}


@misc{Hounslow2013,
author = {Hounslow, Kyle},
booktitle = {Mar 11, 2013},
mendeley-groups = {TCC},
title = {{Real-Time Object Tracking Using OpenCV}},
url = {https://raw.githubusercontent.com/kylehounslow/opencv-tuts/master/object-tracking-tut/objectTrackingTut.cpp},
howpublished={\url{https://www.youtube.com/watch?v=bSeFrPrqZ2A}},
year = {Acesso em: 05 de Dezembro de 2015}
}


@misc{NVIDIA,
author = {NVIDIA},
mendeley-groups = {TCC},
title = {CUDA™ - Plataforma Paralela de Computação},
howpublished={\url{http://www.nvidia.com.br/object/cuda_home_new_br.html}},
urldate = {2015-03-03},
year = {Acesso em: 03 de Março de 2015}
}


@article{Filho2014,
author = {Filho, Jos{\'{e}} Luiz Boanova},
journal = {Revista Brasileira de Direito Aeron{\'{a}}utico e Espacial},
mendeley-groups = {TCC},
title = {{Aeronaves N{\~{a}}o Tripul{\'{a}}veis no Brasil e sua Regula{\c{c}}{\~{a}}o}},
url = {http://www.sbda.org.br/revista/1868.pdf},
year = {2014}
}


@misc{DECEA2015,
author = {{Departamento de Controle do Espa{\c{c}}o A{\'{e}}reo da Aeron{\'{a}}utica}},
booktitle = {Nov 27,2015},
mendeley-groups = {TCC},
title = {{DECEA publica nova regulamenta{\c{c}}{\~{a}}o para voos de RPAS (drones)}},
howpublished={\url{http://www.decea.gov.br/?p=8465}},
urldate = {Dec 6, 2015},
year = {Acesso em: 06 de Dezembro de 2015}
}


@misc{eLinuxJetsonOpenCV,
author = {Wiki, Embedded Linux},
keywords = {CUDA™,GPU,Jetson TK1,OpenCV},
mendeley-groups = {TCC},
title = {{Installing OpenCV (including the GPU module) on Jetson TK1}},
howpublished={\url{http://elinux.org/Jetson/Installing_OpenCV}},
year={Acesso em: 16 de Março de 2016}
}


@phdthesis{Mendes2012,
author = {Mendes, Caio C{\'{e}}sar Teodoro},
file = {:home/nicolas/Desktop/dissertacao{\_}rev{\_}a4-caio.pdf:pdf},
mendeley-groups = {TCC},
school = {University of S{\~{a}}o Paulo},
title = {{Navega{\c{c}}{\~{a}}o de rob{\^{o}}s m{\'{o}}veis utilizando vis{\~{a}}o est{\'{e}}reo}},
year = {2012}
}


@book{RobertLaganiere,
  Author = {Robert Laganière},
  Title = {OpenCV 2 Computer Vision Application Programming Cookbook},
  Publisher = {Packt Publishing},
  Year = {2011},
  ISBN = {1849513244},
  URL = {http://www.amazon.com/OpenCV-Computer-Application-Programming-Cookbook/dp/1849513244%3FSubscriptionId%3D0JYN1NVW651KCA56C102%26tag%3Dtechkie-20%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D1849513244}
}


@misc{DevelopmentTeam2016,
author = {{Development Team}, OpenCV},
booktitle = {OpenCV 2.4.12.0 documentation},
mendeley-groups = {TCC},
title = {{Camera Calibration and 3D Reconstruction}},
howpublished={\url{http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html}},
year = {Acesso em: 30 de Março de 2016}
}


@misc{OpenCVCalibrationModule,
author = {{Development Team}, OpenCV},
booktitle = {OpenCV 2.4.12.0 documentation},
mendeley-groups = {TCC},
title = {{Camera Calibration and 3D Reconstruction}},
howpublished={\url{http://docs.opencv.org/2.4/modules/calib3d/doc/camera_calibration_and_3d_reconstruction.html}},
year = {Acesso em: 30 de Março de 2016}
}


@misc{OpenCVCalibrationTutorial,
author = {{Development Team}, OpenCV},
booktitle = {OpenCV 2.4.13.0 documentation},
mendeley-groups = {TCC},
title = {{Camera Calibration with OpenCV}},
howpublished={\url{http://docs.opencv.org/2.4/doc/tutorials/calib3d/camera_calibration/camera_calibration.html}},
year = {Acesso em: 27 de Abril de 2016}
}


@misc{Bouguet1999,
abstract = {This is a release of a Camera Calibration Toolbox for Matlab® with a complete documentation. This document may also be used as a tutorial on camera calibration since it includes general information about calibration, references and related links.},
author = {Bouguet, Jean-Yves},
booktitle = {Jean-Yves Bouguet's Homepage},
mendeley-groups = {TCC},
title = {{Complete Camera Calibration Toolbox for MATLAB}},
url = {http://www.vision.caltech.edu/bouguetj/},
year = {1999}
}


@article{Scharstein2003,
abstract = {Progress in stereo algorithm performance is quickly outpacing the ability of existing stereo data sets to discriminate among the best-performing algorithms, motivating the need for more challenging scenes with accurate ground truth information. This paper describes a method for acquiring high-complexity stereo image pairs with pixel-accurate correspondence information using structured light. Unlike traditional range-sensing approaches, our method does not require the calibration of the light sources and yields registered disparity maps between all pairs of cameras and illumination projectors. We present new stereo data sets acquired with our method and demonstrate their suitability for stereo algorithm evaluation. Our results are available at http://www.middlebury.edu/stereo/.},
author = {Scharstein, D and Szeliski, R},
doi = {10.1109/CVPR.2003.1211354},
isbn = {0-7695-1900-8},
issn = {1063-6919},
journal = {Computer Vision and Pattern Recognition, 2003. Proceedings. 2003 IEEE Computer Society Conference on},
keywords = {cameras,computer vision,image texture,stereo image processing},
mendeley-groups = {TCC},
number = {June},
pages = {I--195 -- I--202},
title = {{High-accuracy stereo depth maps using structured light}},
volume = {1},
year = {2003}
}


@article{Hirschmuller2008,
abstract = {This paper describes the Semi-Global Matching (SGM) stereo method. It uses a pixelwise, Mutual Information based matching cost for compensating radiometric differences of input images. Pixelwise matching is supported by a smoothness constraint that is usually expressed as a global cost function. SGM performs a fast approximation by pathwise optimizations from all directions. The discussion also addresses occlusion detection, subpixel refinement and multi-baseline matching. Additionally, postprocessing steps for removing outliers, recovering from specific problems of structured environments and the interpolation of gaps are presented. Finally, strategies for processing almost arbitrarily large images and fusion of disparity images using orthographic projection are proposed.A comparison on standard stereo images shows that SGM is among the currently top-ranked algorithms and is best, if subpixel accuracy is considered. The complexity is linear to the number of pixels and disparity range, which results in a runtime of just 1-2s on typical test images. An in depth evaluation of the Mutual Information based matching cost demonstrates a tolerance against a wide range of radiometric transformations. Finally, examples of reconstructions from huge aerial frame and pushbroom images demonstrate that the presented ideas are working well on practical problems.},
author = {Hirschm{\"{u}}ller, Heiko},
doi = {10.1109/TPAMI.2007.1166},
isbn = {0769523722},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Global optimization,Multibaseline,Mutual information,Stereo},
mendeley-groups = {TCC},
number = {2},
pages = {328--341},
pmid = {18084062},
title = {{Stereo processing by semiglobal matching and mutual information}},
volume = {30},
year = {2008}
}


@inproceedings{JunhwanKim2003,
abstract = {We address visual correspondence problems without assuming that scene points have similar intensities in different views. This situation is common, usually due to nonLambertian scenes or to differences between cameras. We use maximization of mutual information, a powerful technique for registering images that requires no a priori model of the relationship between scene intensities in different views. However, it has proven difficult to use mutual information to compute dense visual correspondence. Comparing fixed-size windows via mutual information suffers from the well-known problems of fixed windows, namely poor performance at discontinuities and in low-texture regions. In this paper, we show how to compute visual correspondence using mutual information without suffering from these problems. Using a simple approximation, mutual information can be incorporated into the standard energy minimization framework used in early vision. The energy can then be efficiently minimized using graph cuts, which preserve discontinuities and handle low-texture regions. The resulting algorithm combines the accurate disparity maps that come from graph cuts with the tolerance for intensity changes that comes from mutual information.},
author = {{Junhwan Kim} and Kolmogorov and Zabih},
booktitle = {Proceedings Ninth IEEE International Conference on Computer Vision},
doi = {10.1109/ICCV.2003.1238463},
isbn = {0-7695-1950-4},
mendeley-groups = {TCC},
number = {Iccv},
pages = {1033--1040 vol.2},
title = {{Visual correspondence using energy minimization and mutual information}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1238463},
year = {2003}
}


@article{Monish2011,
abstract = {The ability to rapidly generate traffic predictions is expected to be central for implementing next-generation air traffic management functionality, both on the ground and aboard aircraft. While high-end computers can be used for this purpose, emerging capabilities of computational hardware such as Graphics Processing Units, together with Cloud Computing concepts can be exploited to realize substantial acceleration of trajectory computations at a modest cost increment. This paper discusses the development of a computational appliance for rapid prediction of aircraft trajectories that combines efficient algorithm and software design with emerging high performance computing architectures. The research effort accelerates trajectory predictions through software profiling and tuning, and implements computationally intensive functions on high performance computing architectures such as computing clusters, multi-threaded programming on multi-core computers and Graphics Processing Units. The fastest of these},
author = {Monish, DT and Wiraatmadja, Sandy},
isbn = {9781600869525},
journal = {AIAA Guidance, Navigation, and Control Conference},
mendeley-groups = {TCC},
number = {August},
pages = {1--15},
title = {{High-Speed Prediction of Air Traffic for Real-Time Decision Support}},
url = {http://www.aric.or.kr/treatise/journal/content.asp?idx=145150},
year = {2011}
}


@article{Stone2015,
author = {{E. Stone}, John and {J. Hallock}, Michael and {C. Phillips}, James and {R. Peterson}, Joseph and {L. Vandivort}, Kirby and Luthey-Schulten, Zaida and Schulten, Klaus},
file = {:home/nicolas/Desktop/hcw2016-08-ArmGPU-final.pdf:pdf},
journal = {Parallel and Distributed Processing Symposium Workshop (IPDPSW), 2016 IEEE International},
keywords = {-molecular modeling,1,energy efficiency,fig,gpu computing,heterogeneous architectures,high-performance computing,ing,instrumentation attached to the,mobile comput-,photograph of power monitoring},
mendeley-groups = {TCC},
title = {{Evaluation of Emerging Energy-Efficient Heterogeneous Computing Platforms for Biomolecular and Cellular Simulation Workloads}},
year = {2015}
}


@misc{MatlabStereoApp,
author = {MATLAB - MathWorks Inc.},
mendeley-groups = {TCC},
title = {{Stereo Camera Calibrator}},
howpublished={\url{http://www.mathworks.com/help/vision/ug/stereo-camera-calibrator-app.html}},
urldate = {Apr 20, 2016},
year = {Acesso em: 20 de Abril de 2016}
}


@misc{ItseezOpenCVinfo,
author = {Itseez},
title = {{OpenCV Informations}},
url = {http://itseez.com/opencv/},
howpublished={\url{http://itseez.com/opencv/}},
urldate = {2016-04-20},
year={Acesso em: 20 de Abril de 2016}
}


@misc{ItseezOpenCVPlatforms,
author = {Itseez},
title = {{OpenCV Platforms}},
url = {http://opencv.org/platforms.html},
howpublished={\url{http://opencv.org/platforms.html}},
urldate = {2016-04-20},
year={Acesso em: 20 de Abril de 2016}
}


@article{Magno2009,
author = {Victor M. G. Paula},
booktitle = {03-Aug-2009},
file = {:home/nicolas/Desktop/BQM1BR.pdf:pdf},
journal = {Centro de Pesquisas Estrat{\'{e}}gicas “Paulino Soares de Sousa”},
pages = {1--3},
title = {{O vant {\`{a}} jato brasileiro}},
howpublished={\url{http://www.ecsbdefesa.com.br/defesa/index.php?option=com_content&task=view&id=1677&Itemid=40}},
urldate = {2014-04-20},
year = {Acesso em: 20 de Abril de 2016}
}


@misc{BeagleBoardOrg,
author = {BeagleBoard.org},
booktitle = {BeagleBone Black},
title = {{BeagleBone Black}},
howpublished={\url{https://beagleboard.org/}},
year = {Acesso em: 20 de Abril de 2016}
}


@misc{JetsonTK1,
author = {NVIDIA},
mendeley-groups = {TCC},
title = {{Jetson TK1 Developer Kit}},
howpublished={\url{https://developer.nvidia.com/embedded/develop/hardware}},
year={Acesso em: 20 de Abril de 2016}
}


@misc{JetsonTX1,
author = {NVIDIA},
mendeley-groups = {TCC},
title = {{Jetson TX1 Developer Kit}},
howpublished={\url{http://www.nvidia.com/object/jetson-tx1-dev-kit.html}},
year = {Acesso em: 21 de Maio de 2016}
}


@misc{ARMNEON,
author = {ARM Ltd.},
mendeley-groups = {TCC},
title = {{NEON™}},
howpublished={\url{http://www.arm.com/products/processors/technologies/neon.php}},
year={Acesso em: 23 de Maio de 2016}
}


@misc{Harasimowicz2015,
author = {Harasimowicz, Adam},
mendeley-groups = {TCC},
title = {{Install OpenCV with CUDA}},
howpublished={\url{http://blog.aicry.com/ubuntu-14-04-install-opencv-with-cuda/}},
urldate = {2016-05-24},
year = {Acesso em: 24 de Maio de 2016}
}


@misc{FacebookCUDA,
author = {Facebook},
mendeley-groups = {TCC},
title = {{CUDA Installation}},
howpublished={\url{https://github.com/facebook/fbcunn/blob/master/INSTALL.md}},
urldate = {2016-05-24},
year={Acesso em: 24 de Maio de 2016}
}


@inproceedings{Wang2011,
abstract = {This document mainly outlines the procedures on implementation of control of S3C2440 based embedded Linux stepping motor. Such discussion is focused on hardware design method, driving program compilation and application program compilation. It also introduces the concept of stepping motor control and analyzes structure, compilation procedure and improvement of stepping motor driving program in embedded system. Application of S3C2440 and Linux 2.6.30 based driving program can realize various controls by ARM processor of stepping motor in embedded Linux system. ?? 2011 Published by Elsevier B.V.},
author = {Wang, Shuai and Zhang, Heng and Tan, Han Qing and Jiang, Lin Ying},
booktitle = {Energy Procedia},
doi = {10.1016/j.egypro.2012.01.241},
issn = {18766102},
keywords = {ARM processor,Embedded system,Linux 2.6.30,Stepping motor},
mendeley-groups = {TCC},
number = {PART C},
pages = {1541--1546},
title = {{Implementation of step motor control under embedded linux based on S3C2440}},
volume = {16},
year = {2011}
}


@book{Yaghmour2008,
author = {Yaghmour, Karim and Masters, Jon and Ben-Yossef, Gilad and Gerum, Philippe},
isbn = {0596529686},
mendeley-groups = {TCC},
publisher = {O'Reilly Media},
title = {{Building Embedded Linux Systems}},
url = {http://www.amazon.com/Building-Embedded-Linux-Systems-Yaghmour/dp/0596529686{\%}3FSubscriptionId{\%}3D0JYN1NVW651KCA56C102{\%}26tag{\%}3Dtechkie-20{\%}26linkCode{\%}3Dxm2{\%}26camp{\%}3D2025{\%}26creative{\%}3D165953{\%}26creativeASIN{\%}3D0596529686},
year = {2008}
}


@misc{StereoARM,
abstract = {Passive stereo vision is a powerful visual sensing technique aimed at inferring depth without using any structured light. Nowadays, as it offers low cost and reliability solutions, it finds application in many real use cases, such as natural user interfaces, industrial automation, autonomous vehicles, and many more. Since stereo vision algorithms are extremely computationally expensive, resulting in very high CPU load, the aim of this presentation is to demonstrate the feasibility of this task on a low power mobile ARM{\textregistered} Mali™ GPU. In particular, the presentation focuses on a local stereo vision method based on a novel extension of census transform, which exploits the highly parallel execution feature of mobile Graphic Processing Units with OpenCL. The presentation shows also the approaches and the strategies used to optimize the OpenCL™ code in order to reach significant performance benefits on the GPU.},
address = {Cambrigde, UK},
author = {{Gian Marco Iodice; Media Processing Group; ARM}},
file = {:home/nicolas/Desktop/Real-time Dense Passive Stereo Vision A Case Study in Optimizing Computer Vision Applications Using OpenCL on ARM Presentation.pdf:pdf},
mendeley-groups = {TCC},
number = {May},
title = {{Real-time Dense Passive Stereo Vision: A Case Study in Optimizing Computer Vision Applications Using OpenCL™ on ARM®}},
year = {2015}
}


@misc{bumblebee2,
author = {Inc., Point Grey Research},
mendeley-groups = {TCC},
title = {{Bumblebee{\textregistered}2}},
howpublished={\url{https://www.ptgrey.com/bumblebee2-firewire-stereo-vision-camera-systems}},
urldate = {2016-05-24},
year={Acesso em: 24 de Maio de 2016}
}


@misc{StereoLabsZED,
author = {StereoLabs},
mendeley-groups = {TCC},
title = {{ZED - 3D Camera for AR/VR and Autonomous Navigation}},
howpublished={\url{https://www.stereolabs.com/zed/specs/}},
urldate = {2016-05-24},
year={Acesso em: 24 de Maio de 2016}
}

@misc{CodeLaboratoriesDUO,
author = {{Code Laboratories Inc.}},
mendeley-groups = {TCC},
title = {{DUO 3D™}},
howpublished={\url{https://duo3d.com/}},
urldate = {2016-05-25},
year={Acesso em: 25 de Maio de 2016}
}


@misc{NVIDIAStereoLabsPartenership,
author = {NVIDIA},
mendeley-groups = {TCC},
title = {{NVIDIA Jetson Partner Story: Stereolabs}},
howpublished={\url{https://developer.nvidia.com/embedded/learn/success-stories/stereolabs}},
urldate = {2016-05-25},
year={Acesso em: 25 de Maio de 2016}
}

@misc{ROS,
author = {Open Source Robotics Foundation},
mendeley-groups = {TCC},
title = {{ROS - Robot Operating System}},
howpublished={\url{http://www.ros.org/}},
urldate = {2016-05-25},
year={Acesso em: 25 de Maio de 2016}
}


@misc{Gazebo,
author = {Open Source Robotics Foundation},
mendeley-groups = {TCC},
title = {{Gazebo}},
howpublished={\url{http://www.gazebosim.org/}},
urldate = {2016-05-25},
year={Acesso em: 25 de Maio de 2016}
}

